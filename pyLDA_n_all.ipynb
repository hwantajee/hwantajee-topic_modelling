{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "#######################변경###########################\n",
    "NUM_TOPICS = 4\n",
    "NUM_TOPIC_WORDS = 100\n",
    "\n",
    "\n",
    "######################################################\n",
    "\n",
    "def get_highest_topic(topic_list):\n",
    "    highest_topic = 100\n",
    "    highest_prob = 0\n",
    "    for topic, prob in topic_list:\n",
    "        if prob > highest_prob:\n",
    "            highest_prob = prob\n",
    "            highest_topic = topic\n",
    "    return highest_topic, highest_prob\n",
    "\n",
    "\n",
    "def build_doc_word_matrix(docs):\n",
    "    dictionary = corpora.Dictionary(docs)   #젠심에서 제공하는 dictionary를 이용해서 전체 데이터에 들어있는 단어들의 고유한 ID를 매김\n",
    "    corpus = []  #얘는 결국  DTM이 됨. \n",
    "    for doc in docs:\n",
    "        bow = dictionary.doc2bow(doc) #다큐먼트에 사용된 단어가 무엇인지, 또 얼만큼 사용했는지 정보를 이용해서 문서를 벡터로. \n",
    "        corpus.append(bow)\n",
    "\n",
    "    return corpus, dictionary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################파일명변경#################################\n",
    "pre_file_name='C:/hwantajee/TA_buzz_all/TKE_buzz_all_V_utf8.xlsx' \n",
    "df = pd.read_excel(pre_file_name) \n",
    "\n",
    "#########################분석할POS변경#################################\n",
    "df=df[(df['pos']=='NNP')|(df['pos']=='NNG')|(df['pos']=='SL+SN')|(df['pos']=='SN+SL')|(df['pos']=='VVVP')|(df['pos']=='VVVX')|(df['pos']=='VVXR')|(df['pos']=='VA+ETM')|(df['pos']=='VA+EC')|(df['pos']=='VA+EF')|(df['pos']=='VA+EP')|(df['pos']=='VAVX')]\n",
    "\n",
    "\n",
    "unique_uuid_list = df.key.unique().tolist()\n",
    "total_Noun_words = {}\n",
    "\n",
    "for u in unique_uuid_list:\n",
    "\n",
    "    k = (df['key'] == u)\n",
    "    df_ = df.loc[k,:]\n",
    "    \n",
    "    token_list=[]\n",
    "    content_list = df_['content'].to_list()\n",
    "    freq_list=df_['tf'].to_list()\n",
    "    for i,j in list(zip(content_list, freq_list)):\n",
    "        for a in range(j):\n",
    "            token_list.append(i)\n",
    "            \n",
    "    total_Noun_words[u] = {}\n",
    "\n",
    "    total_Noun_words[u]['key']= u\n",
    "    total_Noun_words[u]['words']= token_list\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "key= []\n",
    "documents =[]\n",
    "\n",
    "for k in total_Noun_words.keys():\n",
    "    key.append(total_Noun_words[k]['key'])\n",
    "    documents.append(total_Noun_words[k]['words']) # list of docs: a doc is a list of words\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "corpus, dictionary = build_doc_word_matrix(documents)\n",
    "\n",
    "\n",
    "\n",
    "lda_model = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS,\n",
    "                        id2word=dictionary,\n",
    "                        alpha='auto')\n",
    "\n",
    "#corpus: DTM\n",
    "#print_topic_words(lda_model)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "###################################변경####################################\n",
    "f = open('C:/hwantajee/TA_buzz_all/n_buzz_all_word_topic_4.txt','w') \n",
    "for topic_id in range(lda_model.num_topics):\n",
    "    word_probs = lda_model.show_topic(topic_id, NUM_TOPIC_WORDS)\n",
    "    #print(\"Topic ID: {}\".format(topic_id))\n",
    "    #f.write(str(topic_id)+'\\n')\n",
    "    for word, prob in word_probs:\n",
    "        print(\"\\t{}\\t{}\\t{}\".format(topic_id, word, prob))\n",
    "        f.write(str(topic_id)+'\\t'+str(word)+'\\t'+str(prob)+'\\n')\n",
    "    print(\"\\n\")\n",
    "f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################변경###########################################\n",
    "f = open('C:/hwantajee/TA_buzz_all/n_buzz_all_document_topic_4.txt','w')  \n",
    "for i in range(len(documents)):\n",
    "    #print(key_id[i])\n",
    "    topics_list = lda_model.get_document_topics(corpus[i])\n",
    "    #print(topics_list)\n",
    "    hi_topic, hi_prob = get_highest_topic(topics_list)\n",
    "    print(key[i],hi_topic, hi_prob)\n",
    "    f.write(str(hi_topic)+'\\t'+str(key[i])+'\\t'+ str(hi_prob)+'\\n')\n",
    "f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "lda_data = {'corpus': corpus, 'dictionary':dictionary, 'lda_model':lda_model}\n",
    "\n",
    "#################################변경###########################################\n",
    "with open('n_news_data.pickle', 'wb') as f:\n",
    "    pickle.dump(lda_data,f)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "prepared_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "#################################변경###########################################\n",
    "pyLDAvis.save_html(prepared_data, u'nbuzz_all_LDAvis_4.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
